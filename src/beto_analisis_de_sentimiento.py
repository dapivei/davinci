# -*- coding: utf-8 -*-
"""Copy of BETO-Análisis-de-Sentimiento.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IEN494fdDHd1odHbwrMNcvYKfqUEat-k
"""

!pip -q install transformers

import pandas as pd
import torch
import json
import re
import time


from tqdm import tqdm
from datetime import datetime
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.preprocessing import LabelEncoder

!gdown https://drive.google.com/uc?id=1VxKwlH3xUR3ThIAzw3AWa1EzmQdu8nEN

def predict_sentiment(text, model, tokenizer, label_encoder, device):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    inputs = {key: value.to(device) for key, value in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    # Convertir logits a probabilidades usando softmax
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)
    return probabilities[0].cpu().numpy()

month_dict = {
    'enero': '01', 'febrero': '02', 'marzo': '03', 'abril': '04',
    'mayo': '05', 'junio': '06', 'julio': '07', 'agosto': '08',
    'septiembre': '09', 'octubre': '10', 'noviembre': '11', 'diciembre': '12'
}

def interventions(text):
    pattern = r'([A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\s]+?:\s*[\s\S]*?)(?=[A-ZÁÉÍÓÚÑ][A-ZÁÉÍÓÚÑ\s]+?:|\Z)'
    interventions = re.findall(pattern, text)
    return interventions


def president_interventio(text):
    match = 'PRESIDENTE ANDRÉS MANUEL LÓPEZ OBRADOR:'
    filtered_list = [elem for elem in text if elem.startswith(match)]
    return filtered_list


def convert_to_iso(date_str):
    parts = date_str.split()
    month = month_dict.get(parts[0].lower(), '01')
    day = parts[1].replace(',', '')
    year = parts[2]
    return f"{year}-{month}-{day.zfill(2)}"

tokenizer = BertTokenizer.from_pretrained(
    "ignacio-ave/BETO-nlp-sentiment-analysis-spanish"
    )
model = BertForSequenceClassification.from_pretrained(
    "ignacio-ave/BETO-nlp-sentiment-analysis-spanish"
    )
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

json_file = 'discourse.json'

discourses = pd.read_json(json_file)

discourses['timestamp'] = discourses['entry-date'].apply(convert_to_iso)

discourses['interventions'] = discourses['entry-content'].apply(interventions)

discourses['president'] = discourses['interventions'].apply(president_interventio)

interventions = discourses.explode(
    'interventions', ignore_index=True
)
interventions['speaker'] = interventions['interventions'].astype(str).apply(
    lambda x: x.split(':', 1)[0] if len(x.split(':', 1)) > 1 else x)
interventions['speech'] = interventions['interventions'].astype(str).apply(
    lambda x: x.split(':', 1)[1] if len(x.split(':', 1)) > 1 else x
)
interventions.drop(
    columns=['interventions', 'entry-date', 'president', 'entry-title', 'entry-content', 'entry-url'],
    inplace=True
)

interventions['timestamp'] = pd.to_datetime(interventions['timestamp'])

interventions_2020 = interventions[interventions['timestamp'].dt.year >= 2020]

interventions_2020

# Assuming 'interventions' is your DataFrame containing the data
# Assuming predict_sentiment function is defined

interventions['timestamp'] = pd.to_datetime(interventions['timestamp'])

interventions_2019 = interventions[interventions['timestamp'].dt.year == 2018]
# Define labels
labels = ['N-', 'N', 'NEU', 'NONE', 'P', 'P+']

# Fit label encoder
label_encoder = LabelEncoder().fit(labels)

# Initialize an empty list to store results
results = []

# Loop through speeches with tqdm
for index, row in tqdm(
    interventions_2019.iterrows(),
    total=len(interventions_2019),
    desc="Processing speeches",
    unit="speech"
    ):
    speech = row['speech']
    time_ = row['timestamp']
    probabilities = predict_sentiment(speech, model, tokenizer, label_encoder, device)
    result = {'timestamp': time_, 'speech': speech}
    result.update({label: probability for label, probability in zip(labels, probabilities)})
    results.append(result)

time.sleep(1)

# Create DataFrame
df = pd.DataFrame(results)

# Print or view the DataFrame
print(df)

df



df.to_csv('sentiment_2018.csv')

from google.colab import files
files.download('sentiment_2020.csv')

df=pd.read_csv('sentiment_2018.csv')

df1=pd.read_csv('sentiment_2019.csv')
df2=pd.read_csv('sentiment_2020.csv')

file_path = '/content/sentiment_2018.csv'  # Adjust the path as needed

# Download the file
files.download(file_path)

df = pd.concat([df, df1, df2]).drop('Unnamed: 0', axis=1)

df

file_path = '/content/sentiment.csv'  # Adjust the path as needed

# Download the file
files.download(file_path)

df.to_csv('sentiment.csv', index=False)

# Download the CSV file
files.download('sentiment.csv')

df = pd.csv()

df['P+']

x = np.random.randn(100)
y = np.random.randn(100)

np.corrcoef(df['P+'], y)

merged_df = pd.merge(df1, df2, left_on='timestamp', right_on='Date', how='inner')

# Drop the 'Date' column if you don't need it
merged_df.drop('Date', axis=1, inplace=True)

# Display the merged DataFrame
print(merged_df)

